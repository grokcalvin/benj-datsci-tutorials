{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13047b-a6af-4da9-b63b-ca0c2bf8928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra\n",
    "import numpy as np \n",
    "\n",
    "# data processing\n",
    "import pandas as pd \n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# Algorithms\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8edbf-53a6-4b26-b75a-1a653d2cd05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/titanic_survival/test.csv\")\n",
    "train_df = pd.read_csv(\"data/titanic_survival/train.csv\")\n",
    "#sns.displot(x=train_df[\"Fare\"],kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee6da8a-1a87-43e3-9e56-a2d142e8519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is testing data's significance\n",
    "    #removing data without useful patterns or trends\n",
    "\n",
    "\n",
    "    #adding 2 features.\n",
    "    #why?\n",
    "    #wouldnt knowing the sibcount and if married be treated differently?\n",
    "\n",
    "    #perform data grabbing, only rows with partners & no sibcount, then with partners & siblingcount & siblingcount no partner & no siblingcount and no partner\n",
    "    #how to only grab rows that pass\n",
    "    #This data is significant so dont compound to one feature\n",
    "spouse_df = train_df[(train_df[\"Parch\"] > 0) & (train_df[\"SibSp\"] == 0) ] #23:50 #Not_Survived:Survived\n",
    "partner_and_non_spouse_relatives_df = train_df[(train_df[\"Parch\"] > 0) & (train_df[\"SibSp\"] > 0) ] #80:58\n",
    "non_spouse_relatives_df = train_df[(train_df[\"Parch\"] == 0) & (train_df[\"SibSp\"] > 0) ] #70:70\n",
    "no_relatives = train_df[(train_df[\"Parch\"] == 0) & (train_df[\"SibSp\"] == 0) ] #370:155\n",
    "plt.hist(x=spouse_df[\"Survived\"]) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "04be83d5-affd-40f2-a956-0163b85f7f5a",
   "metadata": {},
   "source": [
    "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n",
    "    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    dataset['Deck'] = dataset['Deck'].map(deck)\n",
    "    dataset['Deck'] = dataset['Deck'].fillna(0)\n",
    "    dataset['Deck'] = dataset['Deck'].astype(int)# we can now drop the cabin feature\n",
    "train_df = train_df.drop(['Cabin'], axis=1)\n",
    "test_df = test_df.drop(['Cabin'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14058528-2f92-4434-9e09-1f6108928be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [train_df,test_df]\n",
    "\n",
    "deck_numeric_dictionary = {\n",
    "    \"A\":1,\n",
    "    \"B\":2,\n",
    "    \"C\":3,\n",
    "    \"D\":4,\n",
    "    \"E\":5,\n",
    "    \"F\":6,\n",
    "    \"G\":7,\n",
    "    \"U\":8\n",
    "}\n",
    "\n",
    "for dataset in data:\n",
    "    #extracting and converting data\n",
    "    #havent tested the sinificance of Deck survival rate\n",
    "\n",
    "\n",
    "    \n",
    "    dataset[\"Deck\"] = dataset[\"Cabin\"].fillna(\"U0\")\n",
    "    dataset[\"Deck\"] = dataset[\"Deck\"].map( lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    dataset[\"Deck\"] = dataset[\"Deck\"].map(deck_numeric_dictionary)\n",
    "    dataset[\"Deck\"] = dataset[\"Deck\"].fillna(0)\n",
    "    print(dataset[\"Deck\"].unique())\n",
    "    dataset[\"Deck\"] = dataset[\"Deck\"].astype(int)\n",
    "    \n",
    "    # test_deck = dataset[dataset[\"Deck\"] == 8.0]\n",
    "    # #8:7\n",
    "    # #12:35\n",
    "    # #24:35\n",
    "    # #8:25\n",
    "    # #8:24\n",
    "    # #5:8\n",
    "    # #2:2\n",
    "    # #480:210\n",
    "    # #plt.hist(x=test_deck[\"Survived\"]) \n",
    "    # dataset[\"Deck\"]\n",
    "\n",
    "    \n",
    "    # dataset[\"Deck_Type_Sum\"] = dataset[\"Deck\"].map(lambda x : type(x))\n",
    "\n",
    "    # #are the numbers not floats but numpy objects? no they displayed as floats\n",
    "    \n",
    "    # print(dataset[\"Deck_Type_Sum\"])\n",
    "    # dataset[\"Deck_Type_Sum\"] = dataset[\"Deck\"].map(lambda x : type(x) == type(float))\n",
    "    # print(dataset[\"Deck_Type_Sum\"])\n",
    "    # print(dataset[\"Deck_Type_Sum\"].sum())\n",
    "    # dataset[\"Deck_Type_Sum\"].values\n",
    "    # dataset[\"Deck_Is_In_0_8_Range\"] = ((dataset[\"Deck\"] >= 0) & (dataset[\"Deck\"] < 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980f877-35c4-4421-8980-ffa72cb0b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Deck\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4023f30-2f2c-4ea0-9fd4-ea16be12f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping data (should be at the end\n",
    "for dataset in data:\n",
    "    dataset = dataset.drop(columns=[\"Ticket\",\"PassengerId\"])\n",
    "\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995ed98-c3df-481f-a84b-e16e111a4a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting data\n",
    "\n",
    "gendermap = {\n",
    "    \"male\":0,\n",
    "    \"female\":1\n",
    "}\n",
    "\n",
    "embarkedmap = {\n",
    "    \"S\":0,\n",
    "    \"C\":1,\n",
    "    \"Q\":2\n",
    "}\n",
    "\n",
    "\n",
    "for dataset in data:\n",
    "    dataset[\"Grouped_Fare\"] = dataset[\"Fare\"].fillna(0)\n",
    "    dataset[\"Grouped_Fare\"] = dataset[\"Grouped_Fare\"].astype(int)\n",
    "    #dont need to make a group because data meaning is the same for both\n",
    "    dataset[\"Sex\"] = dataset[\"Sex\"].map(gendermap)\n",
    "    dataset[\"Embarked\"] = dataset[\"Embarked\"].map(embarkedmap)\n",
    "    dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(0)\n",
    "    dataset[\"Embarked\"] = dataset[\"Embarked\"].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35af52-6b31-4300-b9a7-a47eb3c38ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing format of age feature\n",
    "for dataset in data:\n",
    "    dataset[\"Age_Group\"] = dataset[\"Age\"].fillna(0)\n",
    "    dataset[\"Age_Group\"] = dataset[\"Age_Group\"].astype(int)\n",
    "    \n",
    "    dataset.loc[dataset[\"Age\"] < 11,\"Age_Group\"] = 0\n",
    "    dataset.loc[(dataset[\"Age\"] >= 11) & (dataset[\"Age\"] < 18) ,\"Age_Group\"] = 1\n",
    "    dataset.loc[(dataset[\"Age\"] >= 18) & (dataset[\"Age\"] < 22) ,\"Age_Group\"] = 2\n",
    "    dataset.loc[(dataset[\"Age\"] >= 22) & (dataset[\"Age\"] < 27) ,\"Age_Group\"] = 3\n",
    "    dataset.loc[(dataset[\"Age\"] >= 27) & (dataset[\"Age\"] < 33) ,\"Age_Group\"] = 4\n",
    "    dataset.loc[(dataset[\"Age\"] >= 33) & (dataset[\"Age\"] < 40) ,\"Age_Group\"] = 5\n",
    "    dataset.loc[(dataset[\"Age\"] >= 40) & (dataset[\"Age\"] < 66) ,\"Age_Group\"] = 6\n",
    "    dataset.loc[dataset[\"Age\"] >= 66 ,\"Age_Group\"] = 7\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916469ab-5378-4d1f-bbe9-4b0116578825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making grouped fare feature\n",
    "for dataset in data:\n",
    "    dataset.loc[dataset[\"Fare\"] < 7.91, \"Grouped_Fare\"] = 0\n",
    "    dataset.loc[(dataset[\"Fare\"] >= 7.91) & (dataset[\"Fare\"] < 14.454), \"Grouped_Fare\"] = 1\n",
    "    dataset.loc[(dataset[\"Fare\"] >= 14.454) & (dataset[\"Fare\"] < 31), \"Grouped_Fare\"] = 2\n",
    "    dataset.loc[(dataset[\"Fare\"] >= 31) & (dataset[\"Fare\"] < 99), \"Grouped_Fare\"] = 3\n",
    "    dataset.loc[(dataset[\"Fare\"] >= 99) & (dataset[\"Fare\"] < 250), \"Grouped_Fare\"] = 4\n",
    "    dataset.loc[dataset[\"Fare\"] >= 250, \"Grouped_Fare\"] = 5\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5b533-fe03-491b-9f63-2f96c6c656d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question for calvin. Isnt having a title feature for Mr Mrs and Ms useless becuase that relationship is already present in the data?\n",
    "\n",
    "#retest the significance of parch nad SibSp parch = Parent or children count\n",
    "\n",
    "for dataset in data:\n",
    "    \n",
    "    dataset[\"New_Series\"] = dataset[\"Name\"].map(lambda x: \"1\" if re.compile(\"Master\").search(x) else \"0\" )\n",
    "    print(dataset[\"New_Series\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1b832-2050-4698-811a-cf85b94db999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c4ea18a6-9447-4a33-bac7-0535140cdcd7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9278700-d25c-4f4f-844d-71cb59f498dc",
   "metadata": {},
   "source": [
    "##  We will step through these calculations one line at a time.  Once we understand each line we will understand how to work together to accomplish the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a7d201-9f0a-4549-b4fd-d7ee9d3e705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = train_df.isnull().sum().sort_values(ascending=False)\n",
    "percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "missing_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80176822-8983-4ad6-adee-7fd459783743",
   "metadata": {},
   "source": [
    "###  Deep dive into this code snippet from above:\n",
    "\n",
    "```\n",
    "total = train_df.isnull().sum().sort_values(ascending=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346c779-1537-4c24-81d0-90b738fba76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd400ea1-5964-447c-b9dc-20f8f92e3334",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_df))\n",
    "print(type(train_df[\"PassengerId\"]))\n",
    "print(type(train_df.isnull()))\n",
    "print(type(train_df.isnull().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb39e2-3674-4a61-8145-7042f269a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb00707-7dd4-439b-acbc-136ac12c367c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9414b3-9c5a-4696-affa-6fdfc0ec3284",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = train_df.isnull().sum().sort_values(ascending=False)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6c7397-b5f8-4032-b451-bcc19a9ab48f",
   "metadata": {},
   "source": [
    "###  Deep dive into this code snippet from above:\n",
    "\n",
    "```\n",
    "percent_1 = train_df.isnull().sum()/train_df.isnull().count()*100\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e99d23-11f7-446a-a2f4-51a7ff9b6a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count_df = train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90d9d71-6434-4ea2-9fb3-3d24f0d2bdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_count_df = train_df.isnull().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead4a0a-eb40-4943-aaca-a2fe651e4131",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231bdfb-16a1-4278-9518-3cb4ae784b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_count_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41037f45-abcd-4ad7-89ce-9d544efbfb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_1 = null_count_df / total_count_df *100\n",
    "percent_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7159cd-92fd-40cd-b8b5-eadbd87055d5",
   "metadata": {},
   "source": [
    "###  Deep dive into this code snippet from above:\n",
    "```\n",
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23869da-3f1d-4ad8-8956-0f59efd8e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "round(percent_1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc22e6b-6423-4538-b92b-6ab9323b6073",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b071be8d-02ae-4442-a7e3-0ccfe401f95f",
   "metadata": {},
   "source": [
    "###  Deep dive into this code snippet from above:\n",
    "\n",
    "```\n",
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc705c6-72b2-47af-b64e-916a6701063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0013a3-1585-4108-a0e6-1eef37e4a094",
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f94716-6771-423f-9f03-847fbbe0c40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = pd.concat([total, percent_2], axis=1)\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50bbd51-bd6a-4850-a26a-95820de5c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n",
    "missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64758d00-a31c-453d-94e4-33eac9fbeef7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
