{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea13047b-a6af-4da9-b63b-ca0c2bf8928f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear algebra\n",
    "import numpy as np \n",
    "\n",
    "# data processing\n",
    "import pandas as pd \n",
    "\n",
    "# data visualization\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "# Algorithms\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8edbf-53a6-4b26-b75a-1a653d2cd05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/titanic_survival/test.csv\")\n",
    "train_df = pd.read_csv(\"data/titanic_survival/train.csv\")\n",
    "#sns.displot(x=train_df[\"Fare\"],kind=\"hist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee6da8a-1a87-43e3-9e56-a2d142e8519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is testing data's significance\n",
    "    #removing data without useful patterns or trends\n",
    "\n",
    "\n",
    "    #adding 2 features.\n",
    "    #why?\n",
    "    #wouldnt knowing the sibcount and if married be treated differently?\n",
    "\n",
    "    #perform data grabbing, only rows with partners & no sibcount, then with partners & siblingcount & siblingcount no partner & no siblingcount and no partner\n",
    "    #how to only grab rows that pass\n",
    "    #This data is significant so dont compound to one feature\n",
    "spouse_df = train_df[(train_df[\"Parch\"] > 0) & (train_df[\"SibSp\"] == 0) ] #23:50 #Not_Survived:Survived\n",
    "partner_and_non_spouse_relatives_df = train_df[(train_df[\"Parch\"] > 0) & (train_df[\"SibSp\"] > 0) ] #80:58\n",
    "non_spouse_relatives_df = train_df[(train_df[\"Parch\"] == 0) & (train_df[\"SibSp\"] > 0) ] #70:70\n",
    "no_relatives = train_df[(train_df[\"Parch\"] == 0) & (train_df[\"SibSp\"] == 0) ] #370:155\n",
    "plt.hist(x=spouse_df[\"Survived\"]) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "04be83d5-affd-40f2-a956-0163b85f7f5a",
   "metadata": {},
   "source": [
    "deck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\n",
    "data = [train_df, test_df]\n",
    "\n",
    "for dataset in data:\n",
    "    dataset['Cabin'] = dataset['Cabin'].fillna(\"U0\")\n",
    "    dataset['Deck'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    dataset['Deck'] = dataset['Deck'].map(deck)\n",
    "    dataset['Deck'] = dataset['Deck'].fillna(0)\n",
    "    dataset['Deck'] = dataset['Deck'].astype(int)# we can now drop the cabin feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14058528-2f92-4434-9e09-1f6108928be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [train_df,test_df]\n",
    "\n",
    "deck_numeric_dictionary = {\n",
    "    \"A\":1,\n",
    "    \"B\":2,\n",
    "    \"C\":3,\n",
    "    \"D\":4,\n",
    "    \"E\":5,\n",
    "    \"F\":6,\n",
    "    \"G\":7,\n",
    "    \"U\":8\n",
    "}\n",
    "\n",
    "for dataset in data:\n",
    "    #extracting and converting data\n",
    "    #havent tested the sinificance of Deck survival rate\n",
    "\n",
    "    dataset[\"Deck\"] = dataset[\"Cabin\"].fillna(\"U0\")\n",
    "    dataset[\"Deck\"] = dataset[\"Deck\"].map( lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n",
    "    dataset[\"Deck\"] = dataset[\"Deck\"].map(deck_numeric_dictionary)\n",
    "    dataset[\"Deck\"] = dataset[\"Deck\"].fillna(0)\n",
    "    print(dataset[\"Deck\"].unique())\n",
    "    dataset[\"Deck\"] = dataset[\"Deck\"].astype(int)\n",
    "    \n",
    "    # test_deck = dataset[dataset[\"Deck\"] == 8.0]\n",
    "    # #8:7\n",
    "    # #12:35\n",
    "    # #24:35\n",
    "    # #8:25\n",
    "    # #8:24\n",
    "    # #5:8\n",
    "    # #2:2\n",
    "    # #480:210\n",
    "    # #plt.hist(x=test_deck[\"Survived\"]) \n",
    "    # dataset[\"Deck\"]\n",
    "\n",
    "    \n",
    "    # dataset[\"Deck_Type_Sum\"] = dataset[\"Deck\"].map(lambda x : type(x))\n",
    "\n",
    "    # #are the numbers not floats but numpy objects? no they displayed as floats\n",
    "    \n",
    "    # print(dataset[\"Deck_Type_Sum\"])\n",
    "    # dataset[\"Deck_Type_Sum\"] = dataset[\"Deck\"].map(lambda x : type(x) == type(float))\n",
    "    # print(dataset[\"Deck_Type_Sum\"])\n",
    "    # print(dataset[\"Deck_Type_Sum\"].sum())\n",
    "    # dataset[\"Deck_Type_Sum\"].values\n",
    "    # dataset[\"Deck_Is_In_0_8_Range\"] = ((dataset[\"Deck\"] >= 0) & (dataset[\"Deck\"] < 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980f877-35c4-4421-8980-ffa72cb0b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"Deck\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4023f30-2f2c-4ea0-9fd4-ea16be12f77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping data (should be at the end\n",
    "for dataset in data:\n",
    "    dataset = dataset.drop(columns=[\"Ticket\",\"PassengerId\"])\n",
    "\n",
    "print(train_df)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995ed98-c3df-481f-a84b-e16e111a4a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting data\n",
    "\n",
    "gendermap = {\n",
    "    \"male\":0,\n",
    "    \"female\":1\n",
    "}\n",
    "\n",
    "embarkedmap = {\n",
    "    \"S\":0,\n",
    "    \"C\":1,\n",
    "    \"Q\":2\n",
    "}\n",
    "\n",
    "\n",
    "for dataset in data:\n",
    "    #dont need to make a group because data meaning is the same for both\n",
    "    dataset[\"Sex\"] = dataset[\"Sex\"].map(gendermap)\n",
    "    dataset[\"Embarked\"] = dataset[\"Embarked\"].map(embarkedmap)\n",
    "    dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(0)\n",
    "    dataset[\"Embarked\"] = dataset[\"Embarked\"].astype(int)\n",
    "\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935e8951-b178-4032-921c-ab1bc62f6549",
   "metadata": {},
   "source": [
    "min =0\n",
    "max =20\n",
    "age_df = train_df[(train_df[\"Age\"] >= min) & (train_df[\"Age\"] < max)]\n",
    "plt.hist(x=age_df[\"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35af52-6b31-4300-b9a7-a47eb3c38ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing format of age feature\n",
    "for dataset in data:\n",
    "    dataset[\"Age_Group\"] = dataset[\"Age\"].fillna(0)\n",
    "    dataset[\"Age_Group\"] = dataset[\"Age_Group\"].astype(int)\n",
    "    \n",
    "    dataset.loc[dataset[\"Age\"] < 11,\"Age_Group\"] = 0\n",
    "    dataset.loc[(dataset[\"Age\"] >= 11) & (dataset[\"Age\"] < 18) ,\"Age_Group\"] = 1\n",
    "    dataset.loc[(dataset[\"Age\"] >= 18) & (dataset[\"Age\"] < 22) ,\"Age_Group\"] = 2\n",
    "    dataset.loc[(dataset[\"Age\"] >= 22) & (dataset[\"Age\"] < 27) ,\"Age_Group\"] = 3\n",
    "    dataset.loc[(dataset[\"Age\"] >= 27) & (dataset[\"Age\"] < 33) ,\"Age_Group\"] = 4\n",
    "    dataset.loc[(dataset[\"Age\"] >= 33) & (dataset[\"Age\"] < 40) ,\"Age_Group\"] = 5\n",
    "    dataset.loc[(dataset[\"Age\"] >= 40) & (dataset[\"Age\"] < 66) ,\"Age_Group\"] = 6\n",
    "    dataset.loc[dataset[\"Age\"] >= 66 ,\"Age_Group\"] = 7\n",
    "\n",
    "print(train_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f720ccfc-0090-4bb3-85f6-44778501fb03",
   "metadata": {},
   "source": [
    "data = [train_df, test_df]\n",
    "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "\n",
    "for dataset in data:\n",
    "    # extract titles\n",
    "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    # replace titles with a more common title or as Rare\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n",
    "                                            'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "    # convert titles into numbers\n",
    "    dataset['Title'] = dataset['Title'].map(titles)\n",
    "    # filling NaN with 0, to get safe\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "train_df = train_df.drop(['Name'], axis=1)\n",
    "test_df = test_df.drop(['Name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207de616-b0b1-4688-8e81-6ca93e110592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retest the significance of parch nad SibSp parch = Parent or children count\n",
    "\n",
    "data = [train_df, test_df]\n",
    "titles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "\n",
    "for dataset in data:\n",
    "    # extract titles\n",
    "                                                    #this only selects a group in the setting of the outside of the group.\n",
    "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    print(dataset[\"Title\"])\n",
    "    #print(is \" \" in d)\n",
    "    # replace titles with a more common title or as Rare\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "    # convert titles into numbers\n",
    "    dataset['Title'] = dataset['Title'].map(titles)\n",
    "    # filling NaN with 0, to get safe\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "    dataset[\"Title\"] = dataset[\"Title\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916469ab-5378-4d1f-bbe9-4b0116578825",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making grouped fare feature\n",
    "for dataset in data:\n",
    "    dataset[\"Grouped_Fare\"] = dataset[\"Fare\"].fillna(0)\n",
    "    dataset[\"Grouped_Fare\"] = dataset[\"Grouped_Fare\"].astype(int)\n",
    "    dataset.loc[dataset[\"Fare\"] < 7.91, \"Grouped_Fare\"] = 0\n",
    "    dataset.loc[(dataset[\"Fare\"] >= 7.91) & (dataset[\"Fare\"] < 14.454), \"Grouped_Fare\"] = 1\n",
    "    dataset.loc[(dataset[\"Fare\"] >= 14.454) & (dataset[\"Fare\"] < 31), \"Grouped_Fare\"] = 2\n",
    "    dataset.loc[(dataset[\"Fare\"] >= 31) & (dataset[\"Fare\"] < 99), \"Grouped_Fare\"] = 3\n",
    "    dataset.loc[(dataset[\"Fare\"] >= 99) & (dataset[\"Fare\"] < 250), \"Grouped_Fare\"] = 4\n",
    "    dataset.loc[dataset[\"Fare\"] >= 250, \"Grouped_Fare\"] = 5\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1b832-2050-4698-811a-cf85b94db999",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in data:\n",
    "    #this assumes that one person pays for all relatives and partner.\n",
    "    #how likely are 2+ people to be old and have siblings aboard?\n",
    "    new_df = dataset[(dataset[\"Age\"] >= 18) & ((dataset[\"SibSp\"] >= 2) | (dataset[\"Title\"] == 2))]\n",
    "    print(len(new_df)/len(dataset)*100)\n",
    "    #comb throught the data, try to understand it\n",
    "    # for num in range(6):\n",
    "    #     print(\"*\"*10)\n",
    "    #     print(dataset[\"Fare\"][(dataset[\"Deck\"] == num) & ((dataset[\"SibSp\"] == 0) & (dataset[\"Parch\"] == 0))])\n",
    "    # print(\"*\"*30)\n",
    "    # for num in range(6):\n",
    "    #     print(\"*\"*10)\n",
    "    #     print(dataset[\"Fare\"][(dataset[\"Deck\"] == num) & ((dataset[\"SibSp\"] != 0) | (dataset[\"Parch\"] != 0))])\n",
    "\n",
    "#test relatives\n",
    "for dataset in data:\n",
    "    dataset['Fare_Per_Person'] = dataset['Fare']/((dataset['SibSp']) + (dataset['Parch']))\n",
    "    dataset.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "    dataset[\"Fare_Per_Person\"] = dataset[\"Fare_Per_Person\"].fillna(0)\n",
    "    print(dataset[\"Fare_Per_Person\"])\n",
    "\n",
    "    dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)\n",
    "    print(dataset[\"Fare_Per_Person\"])\n",
    "\n",
    "\n",
    "    #print(dataset[\"Fare\"][(dataset[\"Deck\"] == 1) & (dataset[\"Parch\"] > 0)])\n",
    "    #deck 2 has significantly higher \n",
    "    #does fare cost co inside with deck?\n",
    "        #yes but the majority of the values stay in a set range.\n",
    "    #how does machine learning handle larger values?\n",
    "\n",
    "#     dataset['Fare_Per_Person'] = dataset['Fare']/((dataset['relatives']+1)+(dataset['Parch']))\n",
    "#     dataset['Fare_Per_Person'] = dataset['Fare_Per_Person'].astype(int)# Let's take a last look at the training set, before we start training the models.\n",
    "# train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90853c53-2029-4295-bec6-0ddfa4160992",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns = [\"Name\", \"PassengerId\",\"Ticket\",\"Age\",\"Fare\",\"Cabin\",\"Parch\"])\n",
    "test_df = test_df.drop(columns = [\"Name\", \"PassengerId\",\"Ticket\",\"Age\",\"Fare\",\"Cabin\",\"Parch\"])\n",
    "#count is alvive generalizes data, is that something you want?\n",
    "\n",
    "print(train_df)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf98fa-0627-472e-a3d6-9a5b77b1a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train_df.drop(\"Survived\",axis=1)\n",
    "y_train = train_df[\"Survived\"]\n",
    "\n",
    "x_test = test_df\n",
    "\n",
    "# #no connection to the data. its simply grouped\n",
    "# no parch but sibsp = good\n",
    "# sibsp but no parch = good\n",
    "# output of no parch but sibsp and output of sibsp and no parch = same. except more specific\n",
    "# simplied value, no need for ranges or many leaves.\n",
    "# when has a range of numbers many leaves form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899822b-4667-4ac1-a6e1-ad1e3bb7f0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find out whats benefictual n_estimators based on the data.\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "#what does predict do?\n",
    "#change attributes of the RandomForestClassifier\n",
    "#the perdiction does effect the .score function\n",
    "#sometimes code doesnt make sense untl you look at later code.\n",
    "rf_pred = rf.predict(x_test)\n",
    "\n",
    "rf.score(x_train, y_train)\n",
    "acc_random_forest = round(rf.score(x_train, y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec55c20-5763-4110-872a-598dce7921ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = linear_model.SGDClassifier(max_iter=5, tol=None)\n",
    "sgd.fit(x_train, y_train)\n",
    "sgd_pred = sgd.predict(x_test)\n",
    "\n",
    "sgd.score(x_train, y_train)\n",
    "\n",
    "acc_sgd = round(sgd.score(x_train, y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7937f6c2-0038-4403-a711-08cc31d1b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train, y_train)\n",
    "\n",
    "logreg_pred = logreg.predict(x_test)\n",
    "\n",
    "acc_log = round(logreg.score(x_train, y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae0815-6826-4dce-91a3-d6fc337884c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "knn_pred = knn.predict(x_test)\n",
    "acc_knn = round(knn.score(x_train, y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc574dde-3531-4a5f-90fe-e1614b37cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian = GaussianNB()\n",
    "gaussian.fit(x_train, y_train)\n",
    "gaus_pred = gaussian.predict(x_test)\n",
    "\n",
    "acc_gaussian = round(gaussian.score(x_train, y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a45b7-644f-45a3-b028-15ddcb7aaaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# with warnings.catch_warnings():\n",
    "#  # ignore all caught warnings\n",
    "#  warnings.filterwarnings(\"ignore\")\n",
    "#  # execute code that will generate warnings\n",
    "\n",
    "perceptron = Perceptron(max_iter=5)\n",
    "perceptron.fit(x_train, y_train)\n",
    "\n",
    "perceptron_pred = perceptron.predict(x_test)\n",
    "\n",
    "acc_perceptron = round(perceptron.score(x_train, y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c86be-11c4-483e-97d3-3dc4786cc04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_svc = LinearSVC()\n",
    "linear_svc.fit(x_train, y_train)\n",
    "\n",
    "l_svc_pred = linear_svc.predict(x_test)\n",
    "\n",
    "acc_linear_svc = round(linear_svc.score(x_train, y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae3016-a56a-42ed-83cd-143fa35a18d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier()\n",
    "decision_tree.fit(x_train, y_train)\n",
    "dt_pred = decision_tree.predict(x_test)\n",
    "\n",
    "acc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a6b82-9ad2-4457-bf0f-d884bede0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({\n",
    "    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n",
    "              'Random Forest', 'Naive Bayes', 'Perceptron', \n",
    "              'Stochastic Gradient Decent', \n",
    "              'Decision Tree'],\n",
    "    'Score': [acc_linear_svc, acc_knn, acc_log, \n",
    "              acc_random_forest, acc_gaussian, acc_perceptron, \n",
    "              acc_sgd, acc_decision_tree]})\n",
    "result_df = results.sort_values(by='Score', ascending=False)\n",
    "result_df = result_df.set_index('Score')\n",
    "result_df.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf0a602-c51d-452e-9072-d8e01ea23958",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(rf, x_train, y_train, cv=10, scoring = \"accuracy\")\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Mean:\", scores.mean())\n",
    "print(\"Standard Deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0babf51a-5f2b-4d51-ae72-e7e51f8cc7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "importantances = pd.DataFrame({\"features\":x_train.columns,\"importantance\":np.round(rf.feature_importances_,3)})\n",
    "importantances = importantances.sort_values(by='importantance', ascending=False)\n",
    "importantances.head(10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f723fb6e-ff72-4ae7-8482-ba4cc5b02e4d",
   "metadata": {},
   "source": [
    "94.05 \tRandom Forest\n",
    "94.05 \tDecision Tree\n",
    "86.64 \tKNN\n",
    "80.02 \tLogistic Regression\n",
    "79.12 \tSupport Vector Machines\n",
    "77.89 \tStochastic Gradient Decent\n",
    "76.77 \tNaive Bayes\n",
    "72.17 \tPerceptron\n",
    "\n",
    "94.05 \tRandom Forest\n",
    "94.05 \tDecision Tree\n",
    "87.09 \tKNN\n",
    "79.91 \tLogistic Regression\n",
    "79.35 \tSupport Vector Machines\n",
    "79.35 \tPerceptron\n",
    "76.77 \tNaive Bayes\n",
    "66.44 \tStochastic Gradient Decent #this decreased with losing parch? does it have some other method?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
