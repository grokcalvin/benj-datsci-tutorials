{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Lab: Cost Function for Logistic Regression\n",
    "\n",
    "## Goals\n",
    "In this lab, you will:\n",
    "- examine the implementation and utilize the cost function for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_common import  plot_data, sigmoid, dlc\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset \n",
    "Let's start with the same dataset as was used in the decision boundary lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = np.array([[0.5, 1.5], [1,1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])  #(m,n)\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])                                           #(m,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a helper function to plot this data. The data points with label $y=1$ are shown as red crosses, while the data points with label $y=0$ are shown as blue circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f31ea92d164796916d9b31e43f377a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,1,figsize=(4,4))\n",
    "plot_data(X_train, y_train, ax)\n",
    "\n",
    "# Set both axes to be from 0-4\n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "In a previous lab, you developed the *logistic loss* function. Recall, loss is defined to apply to one example. Here you combine the losses to form the **cost**, which includes all the examples.\n",
    "\n",
    "\n",
    "Recall that for logistic regression, the cost function is of the form \n",
    "\n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m} \\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right] \\tag{1}$$\n",
    "\n",
    "where\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)})$ is the cost for a single data point, which is:\n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = -y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) \\tag{2}$$\n",
    "    \n",
    "*  where m is the number of training examples in the data set and:\n",
    "$$\n",
    "\\begin{align}\n",
    "  f_{\\mathbf{w},b}(\\mathbf{x^{(i)}}) &= g(z^{(i)})\\tag{3} \\\\\n",
    "  z^{(i)} &= \\mathbf{w} \\cdot \\mathbf{x}^{(i)}+ b\\tag{4} \\\\\n",
    "  g(z^{(i)}) &= \\frac{1}{1+e^{-z^{(i)}}}\\tag{5} \n",
    "\\end{align}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='ex-02'></a>\n",
    "#### Code Description\n",
    "\n",
    "The algorithm for `compute_cost_logistic` loops over all the examples calculating the loss for each example and accumulating the total.\n",
    "\n",
    "Note that the variables X and y are not scalar values but matrices of shape ($m, n$) and ($𝑚$,) respectively, where  $𝑛$ is the number of features and $𝑚$ is the number of training examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_logistic(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes cost\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters  \n",
    "      b (scalar)       : model parameter\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i],w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost +=  -y[i]*np.log(f_wb_i) - (1-y[i])*np.log(1-f_wb_i)\n",
    "             \n",
    "    cost = cost / m\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function dot in module numpy:\n",
      "\n",
      "dot(...)\n",
      "    dot(a, b, out=None)\n",
      "    \n",
      "    Dot product of two arrays. Specifically,\n",
      "    \n",
      "    - If both `a` and `b` are 1-D arrays, it is inner product of vectors\n",
      "      (without complex conjugation).\n",
      "    \n",
      "    - If both `a` and `b` are 2-D arrays, it is matrix multiplication,\n",
      "      but using :func:`matmul` or ``a @ b`` is preferred.\n",
      "    \n",
      "    - If either `a` or `b` is 0-D (scalar), it is equivalent to :func:`multiply`\n",
      "      and using ``numpy.multiply(a, b)`` or ``a * b`` is preferred.\n",
      "    \n",
      "    - If `a` is an N-D array and `b` is a 1-D array, it is a sum product over\n",
      "      the last axis of `a` and `b`.\n",
      "    \n",
      "    - If `a` is an N-D array and `b` is an M-D array (where ``M>=2``), it is a\n",
      "      sum product over the last axis of `a` and the second-to-last axis of `b`::\n",
      "    \n",
      "        dot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        First argument.\n",
      "    b : array_like\n",
      "        Second argument.\n",
      "    out : ndarray, optional\n",
      "        Output argument. This must have the exact kind that would be returned\n",
      "        if it was not used. In particular, it must have the right type, must be\n",
      "        C-contiguous, and its dtype must be the dtype that would be returned\n",
      "        for `dot(a,b)`. This is a performance feature. Therefore, if these\n",
      "        conditions are not met, an exception is raised, instead of attempting\n",
      "        to be flexible.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    output : ndarray\n",
      "        Returns the dot product of `a` and `b`.  If `a` and `b` are both\n",
      "        scalars or both 1-D arrays then a scalar is returned; otherwise\n",
      "        an array is returned.\n",
      "        If `out` is given, then it is returned.\n",
      "    \n",
      "    Raises\n",
      "    ------\n",
      "    ValueError\n",
      "        If the last dimension of `a` is not the same size as\n",
      "        the second-to-last dimension of `b`.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    vdot : Complex-conjugating dot product.\n",
      "    tensordot : Sum products over arbitrary axes.\n",
      "    einsum : Einstein summation convention.\n",
      "    matmul : '@' operator as method with out parameter.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> np.dot(3, 4)\n",
      "    12\n",
      "    \n",
      "    Neither argument is complex-conjugated:\n",
      "    \n",
      "    >>> np.dot([2j, 3j], [2j, 3j])\n",
      "    (-13+0j)\n",
      "    \n",
      "    For 2-D arrays it is the matrix product:\n",
      "    \n",
      "    >>> a = [[1, 0], [0, 1]]\n",
      "    >>> b = [[4, 1], [2, 2]]\n",
      "    >>> np.dot(a, b)\n",
      "    array([[4, 1],\n",
      "           [2, 2]])\n",
      "    \n",
      "    >>> a = np.arange(3*4*5*6).reshape((3,4,5,6))\n",
      "    >>> b = np.arange(3*4*5*6)[::-1].reshape((5,4,6,3))\n",
      "    >>> np.dot(a, b)[2,3,2,1,2,2]\n",
      "    499128\n",
      "    >>> sum(a[2,3,2,:] * b[1,2,:,2])\n",
      "    499128\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(np.dot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the implementation of the cost function using the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36686678640551745\n"
     ]
    }
   ],
   "source": [
    "w_tmp = np.array([1,1])\n",
    "b_tmp = -3\n",
    "print(compute_cost_logistic(X_train, y_train, w_tmp, b_tmp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**: 0.3668667864055175"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "Now, let's see what the cost function output is for a different value of $w$. \n",
    "\n",
    "* In a previous lab, you plotted the decision boundary for  $b = -3, w_0 = 1, w_1 = 1$. That is, you had `b = -3, w = np.array([1,1])`.\n",
    "\n",
    "* Let's say you want to see if $b = -4, w_0 = 1, w_1 = 1$, or `b = -4, w = np.array([1,1])` provides a better model.\n",
    "\n",
    "Let's first plot the decision boundary for these two different $b$ values to see which one fits the data better.\n",
    "\n",
    "* For $b = -3, w_0 = 1, w_1 = 1$, we'll plot $-3 + x_0+x_1 = 0$ (shown in blue)\n",
    "* For $b = -4, w_0 = 1, w_1 = 1$, we'll plot $-4 + x_0+x_1 = 0$ (shown in magenta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfad43da32d64bffaabcac600d819fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Choose values between 0 and 6\n",
    "x0 = np.arange(0,6)\n",
    "\n",
    "# Plot the two decision boundaries\n",
    "x1 = 3 - x0\n",
    "x1_other = 4 - x0\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(4,4))\n",
    "# Plot the decision boundary\n",
    "ax.plot(x0,x1, c=dlc[\"dlblue\"], label=\"$b$=-3\")\n",
    "ax.plot(x0,x1_other, c=dlc[\"dlmagenta\"], label=\"$b$=-4\")\n",
    "ax.axis([0, 4, 0, 4])\n",
    "\n",
    "# Plot the original data\n",
    "plot_data(X_train,y_train,ax)\n",
    "ax.axis([0, 4, 0, 4])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.title(\"Decision Boundary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from this plot that `b = -4, w = np.array([1,1])` is a worse model for the training data. Let's see if the cost function implementation reflects this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost for b = -3 :  0.36686678640551745\n",
      "Cost for b = -4 :  0.5036808636748461\n"
     ]
    }
   ],
   "source": [
    "w_array1 = np.array([1,1])\n",
    "b_1 = -3\n",
    "w_array2 = np.array([1,1])\n",
    "b_2 = -4\n",
    "\n",
    "print(\"Cost for b = -3 : \", compute_cost_logistic(X_train, y_train, w_array1, b_1))\n",
    "print(\"Cost for b = -4 : \", compute_cost_logistic(X_train, y_train, w_array2, b_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Cost for b = -3 :  0.3668667864055175\n",
    "\n",
    "Cost for b = -4 :  0.5036808636748461\n",
    "\n",
    "\n",
    "You can see the cost function behaves as expected and the cost for `b = -4, w = np.array([1,1])` is indeed higher than the cost for `b = -3, w = np.array([1,1])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "In this lab you examined and utilized the cost function for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
